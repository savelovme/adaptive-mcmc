

\section{Preliminaries}
\label{sec:preliminaries}

Taken from \cite{hofstadler2024sureconvergenceratesadaptive} and \cite{laitinen2024invitationadaptivemarkovchain}

We consider a state space $\mathcal{X} $ and a parameter space $\mathcal{I}$, which are assumed to be Polish with countable generated $\sigma$-algebras $\mathcal{F}_{\mathcal{X}}$ and $\mathcal{F}_{\mathcal{I}}$.  
Given a family of transition kernels $\{ {P}_\gamma \}_{\gamma \in \mathcal{I}}$ on $(\mathcal{X}, \mathcal{F}_{\mathcal{X}})$, let  $(X_n, \Gamma_n)_{n \in \mathbb{N}_0}$ be an adaptive MCMC chain evolving in the space $\mathcal{X} \times \mathcal{I}$, where $(\Gamma_n)_{n \in \mathbb{N}_0}$ denotes the sequence of (possibly random) adaptation parameters.
The family $\{P_\gamma\}_{\gamma\in\mathcal{I}}$ must satisfy the following non-restrictive regularity condition: 
\begin{assumption}[Regularity]
    \label{a:regularity}
        $(\gamma,x) \mapsto P_\gamma(x,A)$ is $\mathcal{F}_{\mathcal{I}} \otimes \mathcal{F}_{\mathcal{X}}$-measurable for all $A\in\mathcal{F}_{\mathcal{X}}$.
\end{assumption}
This ensures that $\big((\gamma,x), A\big) \mapsto P_\gamma(x,A)$ defines a Markov transition kernel from $(\mathcal{I}\times\mathcal{X}, \mathcal{F}_{\mathcal{I}} \otimes \mathcal{F}_{\mathcal{X}})$ to $(\mathcal{X}, \mathcal{F}_{\mathcal{X}})$.

We assume that $\pi$ is the invariant distribution of ${P}_\gamma(x, \cdot )$ for any $x \in \mathcal{X}$ and $\gamma \in \mathcal{I}$.
Moreover, we assume that any kernel $P_\gamma$ is $\psi$-irreducible and aperiodic.

For a fixed test function $\varphi\in L^1(\pi)$, we consider an estimator 
\begin{equation}
\label{eq:pi-hat}
    \widehat{\pi}_n(\varphi) := \frac{1}{n}\sum_{k=1}^n \varphi(X_k)
\end{equation}

and investigate the convergence properties
\begin{equation}
\widehat{\pi}_n(\varphi) \xrightarrow{n\to\infty} \pi(\varphi) ,
\label{eq:lln}
\end{equation}

\section{Martingale decomposition}
Following \cite{laitinen2024invitationadaptivemarkovchain}, we make the following two general assumptions:

\begin{assumption}[Iterative structure]
   \label{a:markov}
   There is a filtration $(\mathcal{F}_k)_{k\ge 0}$ such that $(X_k, \Gamma_k)_{k \in \mathbb{N}_0}$ is $(\mathcal{F}_k)_{k \in \mathbb{N}_0}$-adapted, and the following holds for all $k\ge 0$ and all measurable $B \in \mathcal{F}_{\mathcal{X}}$:
   \[
   \prob{X_{k+1}\in B\mid \mathcal{F}_k} = P_{\Gamma_k}(X_k,B) \qquad \text{a.s.}
   \]
\end{assumption}

Natural filtration

\begin{assumption}[Solutions of the Poisson equation]
   \label{a:poisson}
There exists a measurable mapping $(x, \gamma) \mapsto u_{\gamma}(x)$ from $\mathcal{X} \times \mathcal{I}$ to $\mathbb{R}$ which satisfies the following:
\begin{equation}
    \label{eq:poisson}
   u_{\gamma}(x) - (P_{\gamma} u_{\gamma})(x) = \varphi(x) - \pi(\varphi) \qquad \text{for all $y \in \mathcal{X}$ and $\gamma \in \mathcal{I}$}.
\end{equation}
\end{assumption}

Our desired convergence \eqref{eq:lln} can be then analysed using the following decomposition:
\begin{align}
   \sum_{k=1}^n {\varphi(X_k) - \pi(\varphi)} 
   &= 
   \sum_{k=1}^n {u_{\Gamma_k}(X_k) - P_{\Gamma_k}u_{\Gamma_k}(X_k)} \label{eq:main-decomposition} \\
   &= 
   \sum_{k=1}^n {u_{\Gamma_{k-1}}(X_k) - P_{\Gamma_{k-1}}u_{\Gamma_{k-1}}(X_{k-1})} 
   & \big(=: M_n\big) 
   \nonumber\\
   & + 
   \sum_{k=1}^n {u_{\Gamma_k}(X_k) - u_{\Gamma_{k-1}}(X_k)} 
   & \big(=: A_n\big) 
   \nonumber\\
   & + 
   \sum_{k=1}^n {P_{\Gamma_{k-1}}u_{\Gamma_{k-1}}(X_{k-1}) - P_{\Gamma_k}u_{\Gamma_k}(X_k)} &\big(=: R_n\big),
   \nonumber
\end{align}

% \being{equation}
%     \hat{\pi}_n(\varphi) - \pi(\varphi) = \frac{M_n}{n} + \frac{A_n}{n} + \frac{R_n}{n}
% \end{equation}

% The term $R_n$ is a telescoping sum:
% \[ 
% R_n =  P_{\Gamma_0}u_{\Gamma_0}(X_0) - P_{\Gamma_n} u_{\Gamma_n}(X_n), 
% \]
% and therefore, as long as $P_{\Gamma_k}u_{\Gamma_k}(X_k)$ are stable in $k$, the contribution of $R_n/n$ will be negligible for large $n$.

% The term $A_n$ contains the `disturbances' caused by the adaptation, because of the changing Markov transitions. If the adaptation slows down,  typically $A_n/n$ vanishes as $n$ increases, e.g. AIR setting as in \cite{hofstadler2024sureconvergenceratesadaptive}

% Whenever well-defined and suitably regular, $M_n$ is a zero-mean martingale:
% \begin{equation}
%     \mexp{u_{\Gamma_{k-1}}(X_k) - P_{\Gamma_{k-1}}u_{\Gamma_{k-1}}(X_{k-1}) \mid \mathcal{F}_{k-1}}= 0,
%    \label{eq:martingale}
% \end{equation}
% and $M_n/n\to 0$ should often follows easily from a martingale convergence theorem, see \cite{douc2018markov}.




\section{The Poisson equation}

Look into \cite{douc2018markov} for Poisson equation. 

As shown in \cite{hofstadler2024sureconvergenceratesadaptive}, \ref{a:poisson} holds under following assumption on Wasserstein contraction:

\begin{assumption}
\label{a:contraction}
For each $\gamma \in \mathcal{I}$ we assume that $\pi$ is the invariant distribution of $P_\gamma$ and that for all $k \in \mathbb{N}$ 
\[
\tau(P_\gamma^k) \leq C \tau^k,
\]
with $C \in [1, \infty)$ and $\tau \in [0,1)$  independent of $\gamma$.
\end{assumption} 


\begin{proposition}\label{prop:poisson_equation_wasserstein_setting}
Let $\gamma \in \mathcal{I}$, $ \varphi \colon \mathcal{X} \to \mathbb{R}$ be $\pi$ integrable and Lipschitz with constant $L \in (0, \infty)$ and let $E (x) < \infty$ for any $x \in \mathcal{X}$. 
If \ref{a:contraction} holds, then the function $u_\gamma\colon \mathcal{X} \to \mathbb{R}$ given via 
\begin{equation}
    \label{eq:poisson-neumann}
    u_\gamma(x) = \sum_{k=0}^\infty P_\gamma^k \left( \varphi - \pi(\varphi) \right)(x)
\end{equation}

is well defined and we have
\begin{itemize}
\item[i)] $\vert u_\gamma (x) \vert \leq \frac{C \| \varphi \|_{d}}{1- \tau} E(x)$ for any $x \in \mathcal{X}$,
\item[ii)] $\| u_\gamma \|_d \leq \frac{C \| \varphi \|_{d}}{1- \tau} $,
\item[iii)] $u_\gamma (x) - P_\gamma u_\gamma(x) = \varphi(x) - \pi(\varphi)$ for any $x \in \mathcal{X}$.
\end{itemize}

\end{proposition}

We now make an assumption on smoothness w.r.t. parameter $\gamma$. For this purpose, we denote a metric on $\mathcal{I}$ as $\tilde{d}$.
We aim to have Lipschitz-smoothness of mappings $\gamma \mapsto u_{\gamma}(x)$ for any $x \in \mathcal{X}$. The following would suffice to have desired smoothness:
    \[
        \sum_{n=1}^{\infty} \mathcal{W}(\delta_xP_{\gamma}^n, \delta_xP_{\gamma'}^n) \leq \tilde{d}(\gamma, \gamma') \cdot g(x)
    \]
    for all $\gamma, \gamma' \in \mathcal{I}$, $x \in \mathcal{X}$ and some measurable $g \colon \mathcal{X} \to \mathbb{R}$. 
\begin{assumption}
\label{a:gamma-lipshitz}
    There exists $\tilde{L} < \infty$, s.t. for all  $\gamma, \gamma' \in \mathcal{I}$ holds
    \[
    \sup_{\| f \|_d \leq 1} \| (P_{\gamma}-P_{\gamma'})f \|_d \leq \tilde{L} \cdot \tilde{d}(\gamma, \gamma').
    \]
\end{assumption}
% For a weaker assumption, we need to show
% \[
%     \mathcal{W}(\delta_xP_{\gamma}^n, \delta_xP_{\gamma'}^n) \leq \tau \cdot \mathcal{W}(\delta_xP_{\gamma}^{n-1}, \delta_xP_{\gamma'}^{n-1})
% \]


\begin{lemma}
    Under \ref{a:contraction} and \ref{a:gamma-lipshitz} for any $x \in \mathcal{X}$, $\gamma, \gamma' \in \mathcal{I}$ and $n \in \mathbb{N}$ it holds
\[
    \mathcal{W}(\delta_xP_{\gamma}^n, \delta_xP_{\gamma'}^n) \leq n\tau^{n-1} \cdot C^2 \tilde{L}  \tilde{d}(\gamma, \gamma') E(x).
\]
    Consequently, for $u_\gamma$ defined in \eqref{eq:poisson-neumann} we have for any $x \in \mathcal{X}$:
\[
  \sup_{\substack{\gamma, \gamma' \in \mathcal{I} \\ \gamma \neq \gamma'}}\frac{\lvert u_\gamma(x) - u_{\gamma'}(x) \rvert}{\tilde{d}(\gamma, \gamma')} \leq \frac{\tilde{L} C^2 \| \varphi \|_d}{(1-\tau)^2} E(x).
\] 


\begin{proof}

    We observe that for any $n \in \mathbb{N}_0$ and $x \in \mathcal{X}$ it holds
    \begin{align*}
    P_{\gamma}^n\varphi(x)-P_{\gamma'}^n\varphi(x)
    &=\sum_{i=0}^{n-1} \big(P_{\gamma}^{i+1}P_{\gamma'}^{n-(i+1)}\varphi(x)-P_{\gamma}^{i}P_{\gamma'}^{n-i}\varphi(x)\big)  \\
    &=\sum_{i=0}^{n-1} P_{\gamma}^{i}(P_{\gamma}-P_{\gamma'})P_{\gamma'}^{n-i-1}\varphi(x).
    \end{align*}

    Denote $\psi(x) := (P_{\gamma}-P_{\gamma'})P_{\gamma'}^{n-i-1} \varphi(x)$.
    Observing that $\pi(\psi) = \pi P_{\gamma}P_{\gamma'}^{n-i-1}(\varphi) - \pi P_{\gamma'}^{n-i}(\varphi) = 0$ yields

    \begin{align*}
        \left\vert P_{\gamma}^{i}\psi(x) \right\vert 
        &= \left\vert \delta_x P_{\gamma}^{i}(\psi) - \pi(\psi) \right\vert \\
        &\leq \| \psi \|_d \cdot \mathcal{W}(\delta_x P_{\gamma}^i, \pi) \\
        &\leq \| \psi \|_d \cdot C\tau^i \mathcal{W}(\delta_x, \pi) \\
        &= \| \psi \|_d \cdot C\tau^i E(x).
    \end{align*}
    
    Assumption \ref{a:gamma-lipshitz} allows to write
    \begin{align*}
        \| \psi \|_d 
        &= \| (P_{\gamma}-P_{\gamma'})P_{\gamma'}^{n-i-1} \varphi \|_d \\
        &\leq \| P_{\gamma'}^{n-i-1} \varphi \|_d \cdot \tilde{L}\tilde{d}(\gamma, \gamma') \\
        &\leq \| \varphi \|_d \cdot C\tau^{n-i-1} \cdot \tilde{L}\tilde{d}(\gamma, \gamma').
    \end{align*}

    Hence, we can bound
    \begin{align*}
        \left\vert P_{\gamma}^n\varphi(x)-P_{\gamma'}^n\varphi(x) \right\vert
        &\leq n\tau^{n-1} \cdot \| \varphi \|_d C^2 \tilde{L}  \tilde{d}(\gamma, \gamma') E(x).
    \end{align*}

    It directly follows that
    \begin{align*}
        \mathcal{W}(\delta_xP_{\gamma}^n, \delta_xP_{\gamma'}^n)
        &= \sup_{\| f \|_d \leq 1}  \left\vert P_{\gamma}^n f(x)-P_{\gamma'}^n f(x) \right\vert \\
        &\leq n\tau^{n-1} \cdot C^2 \tilde{L}  \tilde{d}(\gamma, \gamma') E(x).
    \end{align*}
    

    Finally,

    \begin{align*}
        \lvert u_\gamma(x) - u_{\gamma'}(x) \rvert 
        &\leq \sum_{n=1}^{\infty} \left\vert P_{\gamma}^n\varphi(x)-P_{\gamma'}^n\varphi(x) \right\vert \\
        &\leq \left( \sum_{n=1}^{\infty} n\tau^{n-1} \right) \cdot \| \varphi \|_d C^2E(x) \cdot \tilde{L}\tilde{d}(\gamma, \gamma') \\
        &= \tilde{d}(\gamma, \gamma') \cdot \frac{\tilde{L} C^2 \| \varphi \|_d}{(1-\tau)^2} E(x).
    \end{align*}
    
\end{proof}
\end{lemma}

\section{Convergence Analysis}

Here we analyze the terms $M_n$, $A_n$, $R_n$ as defined in \eqref{eq:main-decomposition}.
We initially assume that \ref{a:markov}, \ref{a:contraction} hold, and $u_\gamma$ is given by \eqref{eq:poisson-neumann}.


% \subsection{Bounded coarse diffusion coefficient}

\begin{assumption}
\label{a:bounded-diffusison}
    \[
    \sup_{n \in \mathbb{N}} \mexp{\mathrm{diff}(X_n, P_{\Gamma_n})} = \Lambda < \infty.
    \]
\end{assumption}

\begin{lemma}
    Under \ref{a:bounded-diffusison}, $M_n = \sum_{k=1}^n  u_{\Gamma_{k-1}}(X_k) - P_{\Gamma_{k-1}}u_{\Gamma_{k-1}}(X_{k-1})$ defined in \eqref{eq:main-decomposition} is a martingale w.r.t. $(\mathcal{F}_n)_{n \in \mathbb{N}}$.
\end{lemma}
\begin{proof}
Denote $\Delta_k := u_{\Gamma_{k-1}}(X_k) - P_{\Gamma_{k-1}}u_{\Gamma_{k-1}}(X_{k-1})$ for $k \in \mathbb{N}$.
    \begin{align*}
         \mexp{\lvert \Delta_k \rvert} &=  \mexp{\lvert {u_{\Gamma_{k-1}}(X_k) - P_{\Gamma_{k-1}}u_{\Gamma_{k-1}}(X_{k-1})} \rvert} \\
        &\leq  \mexp{\int_{\mathcal{X}} \lvert u_{\Gamma_{k-1}}(X_k) - u_{\Gamma_{k-1}}(x) \rvert P_{\Gamma_{k-1}}(X_{k-1}, dx) } \\
        &\leq \frac{C \| \varphi \|_{d}}{1- \tau}  \mexp{\int_{\mathcal{X}} d(X_k, x) P_{\Gamma_{k-1}}(X_{k-1}, dx) } \\
        &= \frac{C \| \varphi \|_{d}}{1- \tau}  \mexp{\int_{\mathcal{X}} \int_{\mathcal{X}} d(x', x) 
        P_{\Gamma_{k-1}}(X_{k-1}, dx')
        P_{\Gamma_{k-1}}(X_{k-1}, dx) } \\
        &\leq \frac{C \| \varphi \|_{d}}{1- \tau} \sqrt{ \mexp{\int_{\mathcal{X}} \int_{\mathcal{X}} d(x', x)^2
        P_{\Gamma_{k-1}}(X_{k-1}, dx')
        P_{\Gamma_{k-1}}(X_{k-1}, dx) } }\\
        &\leq \frac{C \| \varphi \|_{d}}{1- \tau} \sqrt{\Lambda} < \infty,
    \end{align*}
     where Jensen's inequality is used in the end
    % \begin{align*}
    %  \mexp{\Delta_k^2} &\leq \frac{C^2 \| \varphi \|_{d}^2}{(1- \tau)^2} \mexp{\int_{\mathcal{X}} \int_{\mathcal{X}} d(x', x)^2
    %     P_{\Gamma_{k-1}}(X_{k-1}, dx')
    %     P_{\Gamma_{k-1}}(X_{k-1}, dx) } \\
    %     &\leq \frac{C^2 \| \varphi \|_{d}^2}{(1- \tau)^2} \Lambda .
    % \end{align*}
    % \[
    % \mexp{\lvert M_n \rvert} \leq \sum_{k=1}^n \mexp{\lvert \Delta_k \rvert} \leq n \cdot \frac{C \| \varphi \|_{d}}{1- \tau} \sqrt{\Lambda} < \infty
    % \]

    Disintegration Thm (See Thm. 8.5 in \cite{kallenberg2021foundations}) yields
    \begin{align*}
        \mexp{\Delta_k \mid \mathcal{F}_{k-1}} &\aseq
        \mexp{ u_{\Gamma_{k-1}}(X_k) \mid \mathcal{F}_{k-1} } - P_{\Gamma_{k-1}}u_{\Gamma_{k-1}}(X_{k-1}) \\
         &\aseq P_{\Gamma_{k-1}}u_{\Gamma_{k-1}}(X_{k-1}) - P_{\Gamma_{k-1}}u_{\Gamma_{k-1}}(X_{k-1})  \\
        &= 0.
    \end{align*}
\end{proof}

\begin{lemma}
Under \ref{a:bounded-diffusison} for $M_n$ defined in \eqref{eq:main-decomposition} it holds
\[
    \frac{M_n}{n} \xrightarrow{\text{a.s.}} 0.
\]
\begin{proof}
    Consider the martingale $V_n=\sum_{k=1}^n\frac{\Delta_k}{k}$.
    The differences $\Delta_k$ are uniformly bounded in $L^2$:
    \begin{align*}
         \mexp{\Delta_k^2} &\leq \frac{C^2 \| \varphi \|_{d}^2}{(1- \tau)^2} \mexp{\int_{\mathcal{X}} \int_{\mathcal{X}} d(x', x)^2
            P_{\Gamma_{k-1}}(X_{k-1}, dx')
            P_{\Gamma_{k-1}}(X_{k-1}, dx) } \\
            &\leq \frac{C^2 \| \varphi \|_{d}^2}{(1- \tau)^2} \Lambda .
    \end{align*}
        
    Because martingale differences are orthogonal, we have
    \begin{align*}
       \mexp{V_n^2} &=
       \sum_{k=1}^n\frac{\mexp{\Delta_k^2}}{k^2} \\
       &\leq \frac{C^2 \| \varphi \|_{d}^2}{(1- \tau)^2} \Lambda \cdot\sum_{k=1}^\infty \frac{1}{k^2} \\
       &= \frac{C^2 \| \varphi \|_{d}^2}{(1- \tau)^2} \Lambda \cdot \frac{\pi^2}{6}.
    \end{align*}
    That is, $V_n$ is a $L^2$-bounded martingale, which converges to an a.s.~finite $V_\infty = \sum_{k=1}^\infty\frac{\Delta_k}{k}$ almost surely (see Corollary E.3.5 in \cite{douc2018markov}). Whenever $V_\infty$ is finite, Kronecker's lemma implies that 
    \begin{equation*}
        \frac{M_n}{n}=\frac{1}{n}\sum_{k=1}^n k \frac{\Delta_k}{k}\xrightarrow{\text{a.s.}} 0.
    \end{equation*}
\end{proof}
\end{lemma}


\begin{assumption}
\label{a:bounded-eccentricity}
    \[
    \sup_{n \in \mathbb{N}} \mexp{E(X_n)^2} = K < \infty.
    \]
\end{assumption}
\begin{lemma}
    Under \ref{a:bounded-eccentricity} for $R_n$ defined in \eqref{eq:main-decomposition} it holds
\[
    \frac{R_n}{n} \xrightarrow{\text{a.s.}} 0, \qquad \frac{R_n}{\sqrt{n}} \xrightarrow{P} 0.
\]
\begin{proof}
    For $p \in \{1, \frac{1}{2}\}$ we have
    \begin{align*}
        \left\vert\frac{R_n}{n^p}\right\vert
        &=\frac{1}{n^p}\big|P_{\Gamma_{n}}u_{\Gamma_{n}}(X_{n})- P_{\Gamma_0}u_{\Gamma_0}(X_0)\big| \\
        &\leq \frac{1}{n^p}\big|P_{\Gamma_{n}}u_{\Gamma_{n}}(X_{n})\big| + 
        \frac{1}{n^p}\big| P_{\Gamma_0}u_{\Gamma_0}(X_0)\big| 
    \end{align*}

Under \ref{a:regularity}, $P_{\Gamma_0}u_{\Gamma_0}(X_0)$ is finite for each $\omega \in \Omega $ and hence the second term converges a.s. and, hence, in probability.

For the first term we bound
\begin{align*}
    \prob{\frac{1}{n^p} \left\vert P_{\Gamma_n}u_{\Gamma_n}(X_n) \right\vert > \varepsilon}
    &\leq \frac{1}{n^{2p} \varepsilon^2} \mexp{P_{\Gamma_n}u_{\Gamma_n}(X_n)^2} \\
    &= \frac{1}{n^{2p} \varepsilon^2} \mexp{\mexp{ u_{\Gamma_n}(X_{n+1}) \mid \mathcal{F}_n }^2} \\
    &\leq \frac{1}{n^{2p} \varepsilon^2} \mexp{\mexp{ u_{\Gamma_n}(X_{n+1})^2 \mid \mathcal{F}_n }} \\
    &\leq \left( \frac{C \| \varphi \|_d}{1 - \tau} \right)^2 \cdot \frac{1}{n^{2p} \varepsilon^2} \mexp{\ E(X_{n+1})^2 } \\
    &\leq \left( \frac{C \| \varphi \|_d}{1 - \tau} \right)^2 \cdot \frac{K}{n^{2p} \varepsilon^2} .
\end{align*}

    This shows $\sum_{n=1}^{\infty}  \prob{\frac{1}{n} \left\vert P_{\Gamma_n}u_{\Gamma_n}(X_n) \right\vert > \varepsilon} < \infty$. By Borel-Cantelli lemma it follows that 
  \[
  \frac{1}{n}\big|P_{\Gamma_{n}}u_{\Gamma_{n}}(X_{n})\big| \xrightarrow{\text{a.s.}} 0.
  \]

  We also have $\prob{\frac{1}{\sqrt{n}} \left\vert P_{\Gamma_n}u_{\Gamma_n}(X_n) \right\vert > \varepsilon}\xrightarrow{n \to \infty} 0$, yielding
  \[
  \frac{1}{\sqrt{n}}\big|P_{\Gamma_{n}}u_{\Gamma_{n}}(X_{n})\big| \xrightarrow{P} 0.
  \]

%     \begin{align*}
%         \left\vert\frac{R_n}{n}\right\vert
%         &=\frac{1}{n}\big|P_{\Gamma_{n}}u_{\Gamma_{n}}(X_{n})- P_{\Gamma_0}u_{\Gamma_0}(X_0)\big| \\
%         &\leq \frac{1}{n}\big|P_{\Gamma_{n}}u_{\Gamma_{n}}(X_{n})- P_{\Gamma_n}u_{\Gamma_n}(X_0)\big| + 
%         \frac{1}{n}\big|P_{\Gamma_{n}}u_{\Gamma_{n}}(X_{0})- P_{\Gamma_0}u_{\Gamma_0}(X_0)\big|
%     \end{align*}
    
%     \begin{align*}
%          \lvert P_{\Gamma_{n}}u_{\Gamma_{n}}(X_{n})- P_{\Gamma_n}u_{\Gamma_n}(X_0)\rvert 
%         &= \left\vert \int_{\mathcal{X}} \int_{\mathcal{X}} \left( u_{\Gamma_{n}}(x) - u_{\Gamma_{n}}(x') \right) P_{\Gamma_{n}}(X_0, dx') P_{\Gamma_{n}}(X_n, dx) \right\vert \\
%         &\leq \frac{C \| \varphi \|_{d}}{1- \tau} \mathcal{W}\left( \delta_{X_0}P_{\Gamma_{n}}, \delta_{X_n}P_{\Gamma_{n}} \right) \\
%         &\leq \frac{C \| \varphi \|_{d}}{1- \tau} \tau\left( P_{\Gamma_{n}} \right) \cdot d(X_0, X_n)
%     \end{align*}

% This can be bounded under \ref{a:contraction} only in AIR scheme:
%     \begin{align*}
%         \mexp{ \left\vert d(X_0, X_n) - E(X_0) \right\vert } 
%         &\leq  \mexp{ \int_{\mathcal{X}} \int_{\mathcal{X}} \left\vert d(X_0, x) -  d(X_0, y)  \right\vert (P_{\Gamma_0} \dots P_{\Gamma_{n-1}})(X_0, dx) \pi (dy) } \\
%         &\leq \mexp{ \int_{\mathcal{X}} \int_{\mathcal{X}} d(x,y) (P_{\Gamma_0} \dots P_{\Gamma_{n-1}})(X_0, dx) \pi (dy) } \\
%         &= \mathcal{W}\left( \delta_{X_0} P_{\Gamma_0} \dots P_{\Gamma_{n-1}} , \pi \right) \\
%         &\leq \prod_{k=1}^n \tau\left( P_{\Gamma_{k-1}} \right) \cdot \mathcal{W}\left( \delta_{X_0} , \pi \right)
%     \end{align*}
    
%     \begin{align*}
%         \lvert P_{\Gamma_{n}}u_{\Gamma_{n}}(X_{0})- P_{\Gamma_0}u_{\Gamma_0}(X_0)\rvert &= \left\vert \int_{\mathcal{X}} \int_{\mathcal{X}} \left( u_{\Gamma_{n}}(x) - u_{\Gamma_{0}}(x') \right) P_{\Gamma_{0}}(X_0, dx') P_{\Gamma_{n}}(X_0, dx) \right\vert \\
%         &\leq \int_{\mathcal{X}} \int_{\mathcal{X}} \left( u_{\Gamma_{n}}(x) - u_{\Gamma_{0}}(x') \right) P_{\Gamma_{0}}(X_0, dx') P_{\Gamma_{n}}(X_0, dx)  \\
%         &\leq \frac{ C \| \varphi \|_{d}}{1 - \tau} \left( \tilde{d}(\Gamma_0, \Gamma_n) + \mathcal{W}\left( \delta_{X_0}P_{\Gamma_{0}}, \delta_{X_0}P_{\Gamma_{n}} \right) \right) \\
%         &\leq  \frac{ C \| \varphi \|_{d}}{1 - \tau} \left( 1 + C\tau \right) \cdot \tilde{d}(\Gamma_0, \Gamma_n)
%     \end{align*}

%         \[
%         \frac{1}{n} \tilde{d}(\Gamma_0, \Gamma_n) \xrightarrow{n\to\infty} 0
%         \]
\end{proof}
\end{lemma}

For $k \in \mathbb{N}$, denote $D_k := \|\tilde{d}(\Gamma_{k-1}, \Gamma_k) \|_2 = \sqrt{\mexp{\tilde{d}(\Gamma_{k-1}, \Gamma_k)^2}}$.
\begin{assumption}
\label{a:adaptation-strong}
 Suppose the following series converges:
\[
\sum_{k=1}^{\infty} \frac{1}{k} D_k =: D < \infty.
\]

\end{assumption}
\begin{assumption}
\label{a:adaptation-weak}
Suppose for some $p > 0$ it holds
\[
\frac{1}{n^p} \sum_{k=1}^n D_k \xrightarrow{n \to \infty} 0.
\]

\end{assumption}


\begin{lemma}
Under \ref{a:gamma-lipshitz}, \ref{a:bounded-eccentricity} and \ref{a:adaptation-strong} it holds
\[
    \frac{A_n}{n} \xrightarrow{\text{a.s.}} 0.
\]
% \[
%     \mexp{ \left\vert\frac{A_n}{n}\right\vert} \leq \frac{\tilde{L} C^2 \| \varphi \|_d}{(1-\tau)^2} \sqrt{K} \cdot \frac{1}{n} \sum_{k=1}^n \sqrt{\mexp{\tilde{d}(\Gamma_{k-1}, \Gamma_k)^2}}.
% \]

\begin{proof}
    \ref{a:gamma-lipshitz} yields
    \begin{align*}
        \left\vert\frac{A_n}{n}\right\vert
        & \leq \frac{1}{n} \sum_{k=1}^n \left\vert u_{\Gamma_k}(X_k) - u_{\Gamma_{k-1}}(X_k) \right\vert \\
        &  \leq \frac{\tilde{L} C^2 \| \varphi \|_d}{(1-\tau)^2} \cdot \frac{1}{n} \sum_{k=1}^n \tilde{d}(\Gamma_{k-1}, \Gamma_k) E(X_k).
    \end{align*}

    Consider $Y_n := \sum_{k=1}^n \frac{1}{k} \tilde{d}(\Gamma_{k-1}, \Gamma_k) E(X_k) $. Since all terms are nonnegative, for each $\omega \in \Omega$ there exists a pointwise limit $Y_{\infty}(\omega) := \lim_{n \to \infty} Y_n(\omega) \in [0, \infty]$.
    
    We apply Cauchy–Schwarz and use \ref{a:bounded-eccentricity} and \ref{a:adaptation-strong}:
    \begin{align*}
        \mexp{Y_n} 
        &\leq \sum_{k=1}^n  \frac{1}{k} \sqrt{\mexp{\tilde{d}(\Gamma_{k-1}, \Gamma_k)^2} \mexp{E(X_k)^2}} \\
        &\leq \sqrt{K} \sum_{k=1}^n \frac{1}{k}  \sqrt{\mexp{\tilde{d}(\Gamma_{k-1}, \Gamma_k)^2}} \\
        &= \sqrt{K} \sum_{k=1}^n \frac{1}{k} D_k\\
        &\leq \sqrt{K} \cdot D < \infty.
    \end{align*}
    By monotone convergence, $Y_{\infty}$ satisfies $\mexp{Y_{\infty}} \leq \sqrt{K} \cdot D$ and hence it is a.s. finite.
    Finally, Kronecker's lemma implies that
    \[
        \frac{1}{n} \sum_{k=1}^n \tilde{d}(\Gamma_{k-1}, \Gamma_k) E(X_k) = \frac{1}{n} \sum_{k=1}^n k \cdot \frac{1}{k} \tilde{d}(\Gamma_{k-1}, \Gamma_k) E(X_k) \xrightarrow{\text{a.s.}} 0. 
    \]

\end{proof}
\end{lemma}

\begin{lemma}
Under \ref{a:gamma-lipshitz}, \ref{a:bounded-eccentricity} and \ref{a:adaptation-weak} (for some $p>0$) it holds:
\[
    \frac{A_n}{n^p} \xrightarrow{L_1} 0.
\]
As consequence, convergence in probability also holds.
\begin{proof}
    \begin{align*}
    \mexp{\left\vert\frac{A_n}{n^p}\right\vert} &\leq \frac{\tilde{L} C^2 \| \varphi \|_d}{(1-\tau)^2} \cdot \frac{1}{n^p}  \sum_{k=1}^n \mexp{\tilde{d}(\Gamma_{k-1}, \Gamma_k) E(X_k)} \\
    &\leq \frac{\tilde{L} C^2 \| \varphi \|_d}{(1-\tau)^2} \cdot \frac{1}{n^p}  \sum_{k=1}^n \sqrt{\mexp{\tilde{d}(\Gamma_{k-1}, \Gamma_k)^2} \mexp{E(X_k)^2}} \\
    &\leq \frac{\tilde{L} C^2 \| \varphi \|_d}{(1-\tau)^2}\sqrt{K} \cdot \frac{1}{n^p} \sum_{k=1}^n D_k \xrightarrow{n \to \infty} 0.
\end{align*}
\end{proof}
\end{lemma}

